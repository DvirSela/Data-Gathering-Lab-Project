{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Job Listings from Monster.com, separated by type of job:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import Remote, ChromeOptions  \n",
    "from selenium.webdriver.chromium.remote_connection import ChromiumRemoteConnection  \n",
    "from selenium_stealth import stealth\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the job types we want to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://www.monster.com/jobs'\n",
    "job_type_names = [  'accounting',\n",
    "                    'administration',\n",
    "                    'banking',\n",
    "                    'finance',\n",
    "                    'research',\n",
    "                    'communications',\n",
    "                    'construction',\n",
    "                    'engineering',\n",
    "                    'science',\n",
    "                    'education',\n",
    "                    'entertainment',\n",
    "                    'environmental',\n",
    "                    'government',\n",
    "                    'healthcare',\n",
    "                    'hospitality',\n",
    "                    'human-resources',\n",
    "                    'it',\n",
    "                    'legal',\n",
    "                    'logistics',\n",
    "                    'manufacturing',\n",
    "                    'marketing',\n",
    "                    'media',\n",
    "                    'military',\n",
    "                    'retail',\n",
    "                    'real-estate',\n",
    "                    'sales',\n",
    "                    'telecommunications',\n",
    "                    'transportation',\n",
    "                    'agriculture',\n",
    "                    'animal-care',\n",
    "                    'art',\n",
    "                    'automotive',\n",
    "                    'aviation',\n",
    "                    'customer-service',\n",
    "                    'facilities',\n",
    "                    'insurance',\n",
    "                    'oil-gas',\n",
    "                    'production',\n",
    "                    'quality-control',\n",
    "                    'security',\n",
    "                    'skilled-trades',\n",
    "                    'social-service',\n",
    "                    'sports',\n",
    "                    ]\n",
    "print(len(job_type_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_details(driver):\n",
    "    \"\"\"\n",
    "    This function will scrape the job details from the job page: title, company, location, description\n",
    "    @param driver: the selenium driver object\n",
    "    @return: a tuple of the job details: title, company, location, description\n",
    "    \"\"\"\n",
    "    h2_title, li_company, li_location, job_desc = '','','','' \n",
    "    soup_desc = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    job_header = soup_desc.find('div',id='job-view-header')\n",
    "    h2_title = job_header.find('h2', attrs={'data-testid':'jobTitle'}).text.strip()\n",
    "    li_company = job_header.find('li', attrs={'data-testid':'company'}).text.strip()\n",
    "    li_location = job_header.find('li', attrs={'data-testid':'jobDetailLocation'}).text.strip()\n",
    "    print(f\"Title: {h2_title}\\nCompany: {li_company}\\nLocation: {li_location}\")\n",
    "    job_desc = soup_desc.find('div', id='svx-job-view-description').text.strip()[11:]\n",
    "    return h2_title, li_company, li_location, job_desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_more_jobs(driver):\n",
    "    \"\"\"\n",
    "    This function will scroll down the page to load more jobs from the website. \n",
    "    @param driver: the selenium driver object\n",
    "    @return: True if more jobs are loaded successfully, False otherwise\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        scroll_div = driver.find_element(By.ID, 'card-scroll-container')\n",
    "        driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", scroll_div)\n",
    "        # now we need to scroll up a little bit so more jobs will be loaded\n",
    "        driver.execute_script(\"arguments[0].scrollTop = 0\", scroll_div)\n",
    "        time.sleep(2)\n",
    "        scroll_div = driver.find_element(By.ID, 'card-scroll-container')\n",
    "\n",
    "        driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", scroll_div)\n",
    "        time.sleep(2)\n",
    "    except Exception as e:\n",
    "        print(f'Error while loading more jobs: {e}')\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_by_attribute(driver, attribute, value):\n",
    "    \"\"\"\n",
    "    This function will click on an element by its attribute and value\n",
    "    @param driver: the selenium driver object\n",
    "    @param attribute: the attribute of the element\n",
    "    @param value: the value of the attribute\n",
    "    @return: True if the element is clicked successfully, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        element = driver.find_element(By.XPATH, f\"//button[@{attribute}='{value}']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", element)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['title', 'company', 'location', 'description'])\n",
    "def add_row(df, row):\n",
    "    \"\"\"\n",
    "    This function will add a row to the dataframe\n",
    "    @param df: the dataframe\n",
    "    @param row: the row to be added\n",
    "    @return: the updated dataframe\n",
    "    \"\"\"\n",
    "    df.loc[len(df)] = row\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just checking if the jobs folder exists, if not create it\n",
    "if not os.path.exists('jobs'):\n",
    "    os.makedirs('jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non remote testing:\n",
    "# save_period = 10\n",
    "# for type in job_type_names[:1]:\n",
    "#     try:\n",
    "#         if os.path.exists(f'jobs/{type}_jobs.csv'):\n",
    "#             print(f'file exists for {type}')\n",
    "#             continue\n",
    "#         # first phase - Load Page and get all jobs\n",
    "#         print(f'Getting jobs for {type}')\n",
    "#         add_url = f'q-{type}-jobs'\n",
    "#         driver.get(f'{base_url}/{add_url}')\n",
    "#         time.sleep(5) # captcha\n",
    "#         load_more_jobs(driver)\n",
    "#         time.sleep(1) # wait for jobs to load\n",
    "#         soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "#         job_list_buttons = soup.find_all('button', attrs={'data-testid':'JobCardButton'})\n",
    "#         print(f'found {len(job_list_buttons)} jobs')\n",
    "#         # second phase - get job details\n",
    "#         try:\n",
    "#             for job_list in job_list_buttons:\n",
    "#                 aria_label = job_list.get('aria-label') \n",
    "#                 click_by_attribute(driver, 'aria-label', aria_label)\n",
    "#                 time.sleep(2)\n",
    "#                 title, company, location, desc = get_job_details(driver)\n",
    "#                 # third phase - add row and then save to file\n",
    "#                 add_row(df, [title, company, location, desc])\n",
    "#                 if len(df) % save_period == 0:\n",
    "#                     df.to_csv(f'jobs/{type}_jobs.csv', index=False)\n",
    "#                     print(f'saved {len(df)} jobs')\n",
    "#         except Exception as e:\n",
    "#             print(f'got error in job details: {e}')\n",
    "#             continue\n",
    "#     except Exception as e:\n",
    "#         print(f'got error in job type {type}: {e}')\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a logging in case something goes wrong and we crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging started\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "# log_filename = \"logging_stealth.txt\"\n",
    "log_filename = \"./logs/monster_jobs.log\"\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format=\"%(message)s\", \n",
    "                    handlers=[\n",
    "                        logging.StreamHandler(),  # Print to console\n",
    "                        logging.FileHandler(log_filename, mode=\"w\")  # Save to file\n",
    "                    ])\n",
    "logging.info(\"Logging started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping using the remote feature of selenium\n",
    "Important variables (username and password of bright data) will be loaded from a .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file already exists for accounting\n",
      "file already exists for administration\n",
      "file already exists for banking\n",
      "file already exists for finance\n",
      "file already exists for research\n",
      "file already exists for communications\n",
      "file already exists for construction\n",
      "file already exists for engineering\n",
      "file already exists for science\n",
      "file already exists for education\n",
      "file already exists for entertainment\n",
      "file already exists for environmental\n",
      "file already exists for government\n",
      "file already exists for healthcare\n",
      "file already exists for hospitality\n",
      "file already exists for human-resources\n",
      "file already exists for it\n",
      "file already exists for legal\n",
      "file already exists for logistics\n",
      "file already exists for manufacturing\n",
      "file already exists for marketing\n",
      "file already exists for media\n",
      "file already exists for military\n",
      "file already exists for retail\n",
      "file already exists for real-estate\n",
      "file already exists for sales\n",
      "file already exists for telecommunications\n",
      "file already exists for transportation\n",
      "file already exists for agriculture\n",
      "file already exists for animal-care\n",
      "file already exists for art\n",
      "file already exists for automotive\n",
      "file already exists for aviation\n",
      "file already exists for customer-service\n",
      "file already exists for facilities\n",
      "file already exists for insurance\n",
      "file already exists for oil-gas\n",
      "file already exists for production\n",
      "file already exists for quality-control\n",
      "file already exists for security\n",
      "file already exists for skilled-trades\n",
      "file already exists for social-service\n",
      "file already exists for sports\n"
     ]
    }
   ],
   "source": [
    "USER = os.getenv('USER')\n",
    "PASS = os.getenv('PASS')\n",
    "\n",
    "AUTH = f'{USER}:{PASS}'\n",
    "SBR_WEBDRIVER = f'https://{AUTH}@brd.superproxy.io:9515'\n",
    "\n",
    "sbr_connection = ChromiumRemoteConnection(SBR_WEBDRIVER, 'goog', 'chrome') \n",
    "options = ChromeOptions()\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('--start-maximized')\n",
    "with Remote(sbr_connection, options=options) as driver:\n",
    "    save_period = 10\n",
    "    types_saved = 0\n",
    "    for type in job_type_names:\n",
    "        try:\n",
    "            if types_saved >= 11:\n",
    "                break # need to stop before bright data stops\n",
    "            if os.path.exists(f'jobs/{type}_jobs.csv'):\n",
    "                # print(f'file already exists for {type}')\n",
    "                logging.info(f'file already exists for {type}')\n",
    "                continue\n",
    "            df = pd.DataFrame(columns=['title', 'company', 'location', 'description'])\n",
    "\n",
    "            # first phase - Load Page and get all jobs\n",
    "            # print(f'Getting jobs for {type}')\n",
    "            logging.info(f'Getting jobs for {type}')\n",
    "            add_url = f'q-{type}-jobs' + '&where='\n",
    "            driver.get(f'{base_url}/{add_url}')\n",
    "            time.sleep(5) # captcha\n",
    "            load_more_jobs(driver)\n",
    "            time.sleep(5) # wait for jobs to load\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            job_list_buttons = soup.find_all('button', attrs={'data-testid':'JobCardButton'})\n",
    "            # print(f'found {len(job_list_buttons)} jobs')\n",
    "            logging.info(f'found {len(job_list_buttons)} jobs')\n",
    "            # print(driver.current_url)\n",
    "            logging.info(driver.current_url)\n",
    "            # second phase - get job details\n",
    "            try:\n",
    "                i=0\n",
    "                for job_list in job_list_buttons:\n",
    "                    aria_label = job_list.get('aria-label') \n",
    "                    click_by_attribute(driver, 'aria-label', aria_label)\n",
    "                    time.sleep(2)\n",
    "                    # print(f'Job {i}')\n",
    "                    logging.info(f'Job {i}')\n",
    "                    i+=1\n",
    "                    title, company, location, desc = get_job_details(driver)\n",
    "                    # third phase - add row and then save to file\n",
    "                    add_row(df, [title, company, location, desc])\n",
    "                    if len(df) % save_period == 0 or len(df) == len(job_list_buttons):\n",
    "                        df.to_csv(f'jobs/{type}_jobs.csv', index=False)\n",
    "                        # print(f'saved {len(df)} jobs')\n",
    "                        logging.info(f'saved {len(df)} jobs')\n",
    "            except Exception as e:\n",
    "                # print(f'got error in job details: {e}')\n",
    "                logging.info(f'got error in job details: {e}')\n",
    "                continue\n",
    "            types_saved += 1\n",
    "        except Exception as e:\n",
    "            # print(f'got error in job type {type}: {e}')\n",
    "            logging.info(f'got error in job type {type}: {e}')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post processing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1207 jobs in the final dataframe\n"
     ]
    }
   ],
   "source": [
    "final_df = pd.DataFrame(columns=['type', 'title', 'company', 'location', 'description'])\n",
    "for job_type in job_type_names:\n",
    "    df = pd.read_csv(f'./jobs/{job_type}_jobs.csv')\n",
    "    df['type'] = job_type\n",
    "    final_df = pd.concat([final_df, df], ignore_index=True)\n",
    "\n",
    "print(f'There are {final_df.shape[0]} jobs in the final dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in final_df.columns:\n",
    "    final_df[col] = final_df[col].str.replace('\\n', ' ')\\\n",
    "                    .str.replace('\\r', ' ')\\\n",
    "                    .str.replace('\\t', ' ')\\\n",
    "                    .str.strip()\n",
    "final_df.to_csv('monster_jobs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
